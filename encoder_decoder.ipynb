{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSz8a/ejs3RP3PbZoJ03S4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burakemretetik/dl_with_py/blob/main/encoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BYkf3cicHFNa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# --- 1. AYARLAR VE SABİTLER ---\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "MAX_LEN = 20  # Çok uzun cümleleri almayalım (eğitim hızı için)\n",
        "NUM_SAMPLES = 30000  # Veri setinden alınacak örnek sayısı\n",
        "N_EPOCHS = 10\n",
        "HIDDEN_DIM = 256\n",
        "EMBED_DIM = 128\n",
        "LR = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. VERİYİ İNDİRME VE HAZIRLAMA ---\n",
        "if not os.path.exists(\"deu.txt\"):\n",
        "    print(\"Veri indiriliyor...\")\n",
        "    os.system(\"wget -q http://www.manythings.org/anki/deu-eng.zip\")\n",
        "    os.system(\"unzip -q -o deu-eng.zip\")\n",
        "    print(\"Veri indirildi.\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?üöäß]+\", r\" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "# Kelime Dağarcığı Sınıfı\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.count = 4\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split():\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.count\n",
        "                self.idx2word[self.count] = word\n",
        "                self.count += 1\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        return [self.word2idx.get(w, 3) for w in sentence.split()]\n",
        "\n",
        "# Veriyi Oku ve İşle\n",
        "eng_vocab = Vocabulary()\n",
        "deu_vocab = Vocabulary()\n",
        "pairs = []\n",
        "\n",
        "print(\"Veri işleniyor...\")\n",
        "with open(\"deu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "\n",
        "    # Hızlı eğitim için ilk N cümleyi alıyoruz\n",
        "    for line in lines[:NUM_SAMPLES]:\n",
        "        parts = line.split(\"\\t\")\n",
        "        if len(parts) >= 2:\n",
        "            eng = clean_text(parts[0])\n",
        "            deu = clean_text(parts[1])\n",
        "\n",
        "            # Çok uzun cümleleri ele\n",
        "            if len(eng.split()) < MAX_LEN and len(deu.split()) < MAX_LEN:\n",
        "                eng_vocab.add_sentence(eng)\n",
        "                deu_vocab.add_sentence(deu)\n",
        "                pairs.append((eng, deu))\n",
        "\n",
        "print(f\"Toplam Cümle: {len(pairs)}\")\n",
        "print(f\"İngilizce Kelime Sayısı: {eng_vocab.count}\")\n",
        "print(f\"Almanca Kelime Sayısı: {deu_vocab.count}\")\n",
        "\n",
        "# Dataset ve DataLoader\n",
        "class TransDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng, deu = self.pairs[idx]\n",
        "        eng_idx = [eng_vocab.word2idx[\"<SOS>\"]] + eng_vocab.encode(eng) + [eng_vocab.word2idx[\"<EOS>\"]]\n",
        "        deu_idx = [deu_vocab.word2idx[\"<SOS>\"]] + deu_vocab.encode(deu) + [deu_vocab.word2idx[\"<EOS>\"]]\n",
        "        return torch.tensor(eng_idx), torch.tensor(deu_idx)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_pad = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    trg_pad = pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
        "    return src_pad, trg_pad\n",
        "\n",
        "train_loader = DataLoader(TransDataset(pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA7TincpHg0V",
        "outputId": "723b5342-6719-4233-ed2d-87c0571f4a68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri işleniyor...\n",
            "Toplam Cümle: 30000\n",
            "İngilizce Kelime Sayısı: 4493\n",
            "Almanca Kelime Sayısı: 7300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. MODEL MİMARİSİ (ENCODER-DECODER) ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(1) # [batch, 1]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(device)\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        input = trg[:, 0] # İlk input <SOS>\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Modeli Başlat\n",
        "enc = Encoder(eng_vocab.count, EMBED_DIM, HIDDEN_DIM)\n",
        "dec = Decoder(deu_vocab.count, EMBED_DIM, HIDDEN_DIM)\n",
        "model = Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # PAD'i yoksay"
      ],
      "metadata": {
        "id": "wCYMCj-7H2lj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. EĞİTİM DÖNGÜSÜ ---\n",
        "print(f\"\\nEğitim Başlıyor ({N_EPOCHS} Epoch)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for src, trg in train_loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        # Loss hesabı (SOS token hariç)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # Clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {epoch_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEEwjc59H4uW",
        "outputId": "041a1d67-db26-4453-deeb-4eb2439c96a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Eğitim Başlıyor (10 Epoch)...\n",
            "Epoch 1 | Loss: 3.9216\n",
            "Epoch 2 | Loss: 2.9634\n",
            "Epoch 3 | Loss: 2.5968\n",
            "Epoch 4 | Loss: 2.3506\n",
            "Epoch 5 | Loss: 2.1637\n",
            "Epoch 6 | Loss: 2.0014\n",
            "Epoch 7 | Loss: 1.8728\n",
            "Epoch 8 | Loss: 1.7573\n",
            "Epoch 9 | Loss: 1.6627\n",
            "Epoch 10 | Loss: 1.5681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence):\n",
        "    model.eval()\n",
        "    sentence = clean_text(sentence)\n",
        "    tokens = [eng_vocab.word2idx.get(w, 3) for w in sentence.split()]\n",
        "    tokens = [1] + tokens + [2]\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [1]\n",
        "    for _ in range(MAX_LEN):\n",
        "        # BURADAKİ HATA DÜZELTİLDİ: .unsqueeze(0) KALDIRILDI\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(trg_tensor, hidden)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        if pred_token == 2:\n",
        "            break\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "    return \" \".join([deu_vocab.idx2word[i] for i in trg_indexes[1:]])\n",
        "\n",
        "print(\"\\n--- Çeviri Testleri ---\")\n",
        "test_sentences = [\"go .\", \"come here .\", \"i am happy .\", \"he plays football .\", \"it is cold .\"]\n",
        "for s in test_sentences:\n",
        "    print(f\"Ing: {s:20} -> Alm: {translate_sentence(s)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0krQpOPH-1f",
        "outputId": "f0a32013-1495-45b1-b95b-42fff9ddf438"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Çeviri Testleri ---\n",
            "Ing: go .                 -> Alm: geh weg !\n",
            "Ing: come here .          -> Alm: komm hier herunter .\n",
            "Ing: i am happy .         -> Alm: ich bin glücklich glücklich .\n",
            "Ing: he plays football .  -> Alm: er will auf dem bett .\n",
            "Ing: it is cold .         -> Alm: es ist kalt kalt .\n"
          ]
        }
      ]
    }
  ]
}